# FIS Chatbot - System Flows Documentation

## ğŸ”„ Complete System Architecture Flows

---

## 1ï¸âƒ£ LLM Chat (Text-to-Text)

### **User Flow**
1. User types message in text input
2. (Optional) User attaches image via ğŸ“ button
3. User clicks send button or presses Enter
4. Message appears in chat with user avatar
5. AI typing indicator shows "FIS is thinking..."
6. AI response streams in real-time word-by-word
7. Response rendered with markdown formatting

### **Technical Flow**

#### **Frontend** (`public/app.js`)
```
handleSend()
â”œâ”€â”€ Validate input (text or image)
â”œâ”€â”€ Build message content
â”‚   â”œâ”€â”€ Text: { role: "user", content: "text" }
â”‚   â””â”€â”€ Image: { role: "user", content: [{ type: "text" }, { type: "image_url" }] }
â”œâ”€â”€ Update conversation in localStorage
â”œâ”€â”€ Render user message bubble
â””â”€â”€ Call sendToAPI()

sendToAPI()
â”œâ”€â”€ POST /api/chat
â”œâ”€â”€ Headers: { "Content-Type": "application/json" }
â”œâ”€â”€ Body: { messages, model, temperature, top_p, max_tokens }
â”œâ”€â”€ Show typing indicator
â”œâ”€â”€ Read SSE stream
â”‚   â”œâ”€â”€ Parse "data: " lines
â”‚   â”œâ”€â”€ Accumulate text chunks
â”‚   â”œâ”€â”€ Render markdown in real-time
â”‚   â””â”€â”€ Handle [DONE] signal
â””â”€â”€ Save final message to localStorage
```

#### **Backend** (`server.js`)
```
POST /api/chat
â”œâ”€â”€ Validate request body
â”œâ”€â”€ Check for insults (filter profanity)
â”œâ”€â”€ Set SSE headers
â”‚   â”œâ”€â”€ Content-Type: text/event-stream
â”‚   â”œâ”€â”€ Cache-Control: no-cache
â”‚   â””â”€â”€ Connection: keep-alive
â”œâ”€â”€ Call OpenRouter API with streaming
â”‚   â”œâ”€â”€ Model: user-selected (gpt-4o, claude-3.5, etc.)
â”‚   â”œâ”€â”€ Messages: conversation history
â”‚   â””â”€â”€ stream: true
â”œâ”€â”€ Stream response chunks
â”‚   â””â”€â”€ For each chunk:
â”‚       â”œâ”€â”€ Extract delta.content
â”‚       â”œâ”€â”€ Send: data: {"content": "..."}\n\n
â”‚       â””â”€â”€ Flush to client
â””â”€â”€ Send: data: [DONE]\n\n
```

#### **Data Flow**
```
User Input â†’ Frontend Validation â†’ localStorage Update â†’ API Request
                                                              â†“
                                                    OpenRouter API
                                                              â†“
SSE Stream â† Response Chunks â† Token Generation â† LLM Processing
     â†“
Frontend Rendering (Markdown) â†’ Chat Bubble â†’ User Sees Response
```

---

## 2ï¸âƒ£ Audio Speech-to-Speech

### **User Flow**
1. User clicks ğŸ¤ microphone button
2. Voice modal opens with settings
3. User selects voice (Alloy, Echo, Fable, etc.)
4. User adjusts personality & speed
5. User clicks large mic button to start recording
6. Status: "Recording... Click to stop" (red pulsing)
7. User speaks their message
8. User clicks mic again to stop
9. Status: "Processing your speech..."
10. Status: "ğŸ”Š AI is responding..."
11. Audio plays in real-time as AI speaks
12. Transcript appears in chat
13. Status: "Click mic to start speaking"

### **Technical Flow**

#### **Frontend** (`public/app.js`)
```
Voice Button Click
â”œâ”€â”€ Open voiceModal
â”œâ”€â”€ User selects voice/personality/speed
â””â”€â”€ User clicks voiceMicBtn

startRecording()
â”œâ”€â”€ Request microphone access
â”‚   â””â”€â”€ navigator.mediaDevices.getUserMedia({ audio: true })
â”œâ”€â”€ Create MediaRecorder
â”‚   â”œâ”€â”€ mimeType: "audio/webm"
â”‚   â””â”€â”€ Collect audio chunks
â”œâ”€â”€ Update UI
â”‚   â”œâ”€â”€ voiceMicBtn.classList.add("recording")
â”‚   â”œâ”€â”€ voiceIndicator.classList.add("recording")
â”‚   â””â”€â”€ Status: "Recording..."
â””â”€â”€ Wait for user to stop

stopRecording()
â”œâ”€â”€ mediaRecorder.stop()
â”œâ”€â”€ Trigger mediaRecorder.onstop
â”‚   â”œâ”€â”€ Create Blob from audioChunks
â”‚   â””â”€â”€ Call sendSpeechToSpeech(audioBlob)
â””â”€â”€ Update status: "Processing..."

sendSpeechToSpeech(audioBlob)
â”œâ”€â”€ Create FormData
â”‚   â”œâ”€â”€ audio: audioBlob
â”‚   â”œâ”€â”€ voice: selectedVoice
â”‚   â”œâ”€â”€ speed: selectedSpeed
â”‚   â””â”€â”€ conversationHistory: JSON
â”œâ”€â”€ POST /api/speech-to-speech
â”œâ”€â”€ Read SSE stream
â”‚   â”œâ”€â”€ Parse streaming chunks
â”‚   â”œâ”€â”€ type: 'text' â†’ Accumulate text
â”‚   â”œâ”€â”€ type: 'audio' â†’ Queue audio chunks
â”‚   â”‚   â””â”€â”€ playAudioStream(audioQueue)
â”‚   â””â”€â”€ type: 'done' â†’ Update history & display
â””â”€â”€ Wait for audio completion

playAudioStream(audioQueue)
â”œâ”€â”€ Create Web Audio API context
â”œâ”€â”€ For each audio chunk:
â”‚   â”œâ”€â”€ Decode base64 PCM16
â”‚   â”œâ”€â”€ Convert to AudioBuffer
â”‚   â”œâ”€â”€ Create BufferSource
â”‚   â”œâ”€â”€ Connect to destination
â”‚   â”œâ”€â”€ Play chunk
â”‚   â””â”€â”€ Wait for chunk to finish
â””â”€â”€ Real-time audio playback
```

#### **Backend** (`server.js`)
```
POST /api/speech-to-speech
â”œâ”€â”€ Validate audio file (multer)
â”œâ”€â”€ Extract parameters
â”‚   â”œâ”€â”€ voice (alloy, echo, fable, onyx, nova, shimmer)
â”‚   â”œâ”€â”€ speed (0.5x - 2.0x)
â”‚   â””â”€â”€ conversationHistory
â”œâ”€â”€ Convert audio to base64
â”œâ”€â”€ Build messages array
â”‚   â””â”€â”€ Add: { role: "user", content: [{ type: "input_audio", input_audio: { data, format: "wav" }}]}
â”œâ”€â”€ Set SSE headers
â”œâ”€â”€ Call OpenAI API
â”‚   â”œâ”€â”€ model: "gpt-4o-audio-preview"
â”‚   â”œâ”€â”€ modalities: ["text", "audio"]
â”‚   â”œâ”€â”€ audio: { voice, format: "pcm16" }
â”‚   â”œâ”€â”€ stream: true (REQUIRED for audio)
â”‚   â””â”€â”€ messages: conversation history
â”œâ”€â”€ Stream response
â”‚   â””â”€â”€ For each chunk:
â”‚       â”œâ”€â”€ delta.content â†’ Send: data: {"type":"text","content":"..."}\n\n
â”‚       â”œâ”€â”€ delta.audio â†’ Send: data: {"type":"audio","data":"..."}\n\n
â”‚       â””â”€â”€ finish_reason â†’ Extract transcript
â”œâ”€â”€ Send final message
â”‚   â””â”€â”€ data: {"type":"done","text":"...","transcript":"...","conversationHistory":[...]}\n\n
â””â”€â”€ End stream
```

#### **Data Flow**
```
User Voice â†’ MediaRecorder â†’ Audio Blob â†’ FormData
                                              â†“
                                    POST /api/speech-to-speech
                                              â†“
                                    Audio â†’ Base64 â†’ OpenAI
                                              â†“
                          SSE Stream â† Audio Chunks â† AI Voice Generation
                                              â†“
                          Frontend â† PCM16 Chunks â† Real-time Playback
                                              â†“
                          Web Audio API â†’ Speakers â†’ User Hears Response
```

---

## 3ï¸âƒ£ Video Generation (PDF to D-ID Video)

### **User Flow**
1. User clicks ğŸ“¹ video button
2. Video modal opens
3. User selects API type (Clips or Expressives)
4. User selects avatar style (Professional, Casual, etc.)
5. User uploads PDF document (drag & drop or click)
6. User enters optional prompt
7. User clicks "Generate Video"
8. Progress indicator shows:
   - â³ Initializing...
   - ğŸ“„ Extracting PDF text...
   - ğŸ¤– Generating script with AI...
   - ğŸ¬ Creating video with D-ID...
   - âœ… Video ready!
9. Video URL and script displayed
10. User can download or view video

### **Technical Flow**

#### **Frontend** (`public/app.js`)
```
Video Button Click
â”œâ”€â”€ Open videoModal
â”œâ”€â”€ User configures settings
â”‚   â”œâ”€â”€ API Type: Clips or Expressives
â”‚   â”œâ”€â”€ Avatar: professional, casual, cartoon, etc.
â”‚   â”œâ”€â”€ Sentiment: (if Expressives) professional, friendly, serious
â”‚   â””â”€â”€ Prompt: optional customization
â”œâ”€â”€ User uploads PDF
â”‚   â”œâ”€â”€ Validate: PDF only
â”‚   â”œâ”€â”€ Validate: Max 20MB
â”‚   â””â”€â”€ Display filename
â””â”€â”€ User clicks "Generate Video"

videoGenerate.click()
â”œâ”€â”€ Create FormData
â”‚   â”œâ”€â”€ document: PDF file
â”‚   â”œâ”€â”€ avatar: selected avatar
â”‚   â”œâ”€â”€ prompt: user prompt
â”‚   â”œâ”€â”€ model: LLM model
â”‚   â”œâ”€â”€ apiType: clips/expressives
â”‚   â””â”€â”€ sentiment: (if expressives)
â”œâ”€â”€ POST /api/doc-to-video
â”œâ”€â”€ Receive job_id
â”œâ”€â”€ Save to localStorage (for recovery)
â””â”€â”€ Start pollVideoJobStatus(job_id)

pollVideoJobStatus(job_id)
â”œâ”€â”€ Poll every 2 seconds
â”œâ”€â”€ GET /api/video-status/:jobId
â”œâ”€â”€ Update progress UI
â”‚   â”œâ”€â”€ status: "processing" â†’ Show progress
â”‚   â”œâ”€â”€ status: "completed" â†’ Show video URL
â”‚   â””â”€â”€ status: "failed" â†’ Show error
â”œâ”€â”€ Max 360 polls (12 minutes)
â””â”€â”€ Clear localStorage on completion
```

#### **Backend** (`server.js`)
```
POST /api/doc-to-video
â”œâ”€â”€ Rate limiting (5 requests/10min per IP)
â”œâ”€â”€ Validate file
â”‚   â”œâ”€â”€ Check: PDF type
â”‚   â”œâ”€â”€ Check: Max 20MB
â”‚   â””â”€â”€ Multer upload
â”œâ”€â”€ Create job
â”‚   â”œâ”€â”€ Generate job_id
â”‚   â”œâ”€â”€ Store in videoJobs Map
â”‚   â””â”€â”€ Set TTL (30 minutes)
â”œâ”€â”€ Return job_id immediately
â””â”€â”€ Process async: processVideoJob(job_id)

processVideoJob(job_id)
â”œâ”€â”€ Update: status = "processing", progress = 10%
â”œâ”€â”€ STEP 1: Extract PDF text
â”‚   â”œâ”€â”€ Use pdf-parse library
â”‚   â”œâ”€â”€ Validate: min 50 chars
â”‚   â”œâ”€â”€ Validate: max 5000 chars (script length)
â”‚   â””â”€â”€ Update: progress = 30%
â”œâ”€â”€ STEP 2: Generate script with LLM
â”‚   â”œâ”€â”€ Call OpenRouter API
â”‚   â”œâ”€â”€ Model: user-selected
â”‚   â”œâ”€â”€ Prompt: "Create 30-60 second video script..."
â”‚   â”œâ”€â”€ Retry with exponential backoff (3 attempts)
â”‚   â”œâ”€â”€ Validate: script length < 500 chars
â”‚   â””â”€â”€ Update: progress = 60%
â”œâ”€â”€ STEP 3: Create D-ID video
â”‚   â”œâ”€â”€ Select API: Clips or Expressives
â”‚   â”œâ”€â”€ Get avatar ID from DID_AVATARS config
â”‚   â”œâ”€â”€ Build request
â”‚   â”‚   â”œâ”€â”€ Clips API: /clips
â”‚   â”‚   â””â”€â”€ Expressives API: /talks
â”‚   â”œâ”€â”€ POST to D-ID API
â”‚   â”œâ”€â”€ Receive result_url
â”‚   â””â”€â”€ Update: progress = 100%
â”œâ”€â”€ Update job: status = "completed"
â””â”€â”€ Store: videoUrl, script, result

GET /api/video-status/:jobId
â”œâ”€â”€ Find job in videoJobs Map
â”œâ”€â”€ Return job data
â”‚   â”œâ”€â”€ status: processing/completed/failed
â”‚   â”œâ”€â”€ progress: 0-100
â”‚   â”œâ”€â”€ videoUrl: (if completed)
â”‚   â”œâ”€â”€ script: (if completed)
â”‚   â””â”€â”€ error: (if failed)
â””â”€â”€ Auto-cleanup after TTL (30 min)
```

#### **Data Flow**
```
PDF Upload â†’ Frontend Validation â†’ FormData
                                       â†“
                              POST /api/doc-to-video
                                       â†“
                              Create Job â†’ Return job_id
                                       â†“
                              Background Processing:
                                       â†“
PDF â†’ pdf-parse â†’ Text Extraction â†’ Validation
                                       â†“
Text â†’ OpenRouter API â†’ LLM Script Generation â†’ Retry Logic
                                       â†“
Script â†’ D-ID API (Clips/Expressives) â†’ Video Creation
                                       â†“
Video URL â† Job Completion â† Status Updates â† Progress Tracking
     â†“
Frontend Polling â†’ Display Video â†’ User Downloads
```

---

## ğŸ”§ Key Technical Components

### **State Management**
- **Frontend**: localStorage for conversations, pending jobs
- **Backend**: In-memory Map for video jobs (30min TTL)

### **API Integrations**
1. **OpenRouter**: LLM chat, script generation
2. **OpenAI**: Speech-to-speech (gpt-4o-audio-preview)
3. **D-ID**: Video generation (Clips & Expressives APIs)

### **Streaming Technologies**
- **SSE (Server-Sent Events)**: Text chat, speech-to-speech
- **Web Audio API**: Real-time audio playback
- **MediaRecorder API**: Voice recording

### **Error Handling**
- Rate limiting (video: 5/10min)
- File validation (PDF, 20MB max)
- Retry logic (OpenRouter: 3 attempts, exponential backoff)
- Job recovery (localStorage for page refresh)
- Structured logging (JSON format)

### **Security**
- Helmet.js (CSP, XSS protection)
- Profanity filter (insult detection)
- CORS enabled
- File size limits
- IP-based rate limiting

---

## ğŸ“Š Performance Metrics

### **Response Times**
- **Text Chat**: ~1-3s (streaming starts immediately)
- **Speech-to-Speech**: ~2-5s (real-time audio streaming)
- **Video Generation**: ~30-90s (async with progress updates)

### **Resource Usage**
- **Memory**: In-memory job store with TTL cleanup
- **Bandwidth**: Streaming reduces initial payload
- **Storage**: No persistent storage (all in-memory)

---

## ğŸš€ Next Steps for Production

1. **Persistent Storage**: Redis/PostgreSQL for jobs
2. **Webhooks**: D-ID webhook support
3. **Cost Tracking**: API usage monitoring
4. **Concurrent Limits**: Max jobs per user
5. **Multilingual**: Script length per language
6. **Testing**: Unit tests, integration tests, E2E tests

---

*Last Updated: Feb 17, 2026*
